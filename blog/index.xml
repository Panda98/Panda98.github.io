<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Panda&#39;s Blog</title>
    <link>https://panda98.github.io/blog/</link>
    <description>Recent content in Blogs on Panda&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <copyright>true</copyright>
    <lastBuildDate>Sat, 27 Jan 2018 15:13:09 +0800</lastBuildDate>
    
	<atom:link href="https://panda98.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Python爬虫-Scrapy进阶</title>
      <link>https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-scrapy%E8%BF%9B%E9%98%B6/</link>
      <pubDate>Sat, 27 Jan 2018 15:13:09 +0800</pubDate>
      
      <guid>https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-scrapy%E8%BF%9B%E9%98%B6/</guid>
      <description>参考资料：http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/items.html
Items  提供了类似于字典的API以及用于声明可用字段的简单语法  声明Item import scrapy class Product(scrapy.Item): name = scrapy.Field() price = scrapy.Field() stock = scrapy.Field() last_updated = scrapy.Field(serializer=str)  Item字段  Field对象指明了每个字段的元数据  Item相关API  见参考资料  Item对象 class scrapy.item.Item([arg])   返回一个根据给定的参数可选初始化的item  字段（Field）对象  Field仅仅是内置的dict类的一个别名，并没有提供额外的方法或属性 即，Field完完全全就是Python字典dict  Spiders  Spider类定义了如何爬取某个网站。包括爬取的动作以及如何从网页的内容中提取结构化数据（item）
 spider爬取的循环：
 以初始的URL初始化Request，并设置回调函数。 当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。  spider中初始的request是通过调用 start_requests()来获取的。 start_requests() 读取 start_urls中的URL， 并以 parse为回调函数生成 Request 。
 在回调函数内（parse）分析返回的(网页)内容，返回 Item对象或者 Request或者一个包括二者的可迭代容器。 返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。
 在回调函数内，您可以使用 选择器(Selectors) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。</description>
    </item>
    
    <item>
      <title>Python爬虫-Scrapy初涉</title>
      <link>https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-scrapy%E5%88%9D%E6%B6%89/</link>
      <pubDate>Sat, 27 Jan 2018 15:01:16 +0800</pubDate>
      
      <guid>https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-scrapy%E5%88%9D%E6%B6%89/</guid>
      <description>简介  python开发的一个快速、高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据 可用于数据挖掘、检测和自动化测试 运行流程：  引擎从调度器中取出一个url用于接下来的抓取 引擎把url封装成一个Request传给下载器 下载器把资源下载下载，并封装成Response 爬虫解析Response 解析出实体，则交给实体管道进行进一步的处理 解析出的是url，把url交给调度器等待抓取   快速使用Scrapy 创建项目  在要存放项目的文件夹下打开cmd，输入   scrapy startproject project_name   可以修改settings.py，建议取消下面几行的注释：  # Enable and configure HTTP caching (disabled by default) # See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings HTTPCACHE_ENABLED = True HTTPCACHE_EXPIRATION_SECS = 0 HTTPCACHE_DIR = &#39;httpcache&#39; HTTPCACHE_IGNORE_HTTP_CODES = [] HTTPCACHE_STORAGE = &#39;scrapy.extensions.httpcache.FilesystemCacheStorage&#39;   作用：Scrapy会缓存你有的Requests!当你再次请求时，如果存在缓存文档则返回缓存文档，而不是去网站请求，这样既加快了本地调试速度，也减轻了 网站的压力  在Pycharm中使用Scrapy项目  目前没有找到直接在Pycharm中创建Scrapy项目的途径，所以先自行用命令行创建项目，再在Pycharm中打开
 在project_name下创建文件main.py
  # main.py from scrapy import cmdline cmdline.</description>
    </item>
    
    <item>
      <title>Python爬虫-初涉</title>
      <link>https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-%E5%88%9D%E6%B6%89/</link>
      <pubDate>Sat, 27 Jan 2018 14:56:27 +0800</pubDate>
      
      <guid>https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-%E5%88%9D%E6%B6%89/</guid>
      <description>基本框架 import urllib2 # 设置header user_agent = &#39;Mozilla&#39; headers = {&#39;User-Agent&#39;: user_agent} url = &#39;www.panda.com&#39; # 使用requests获得网页内容 page = requests.get(url, headers=headers).content # 使用urllib2获得网页内容 # Request(url,data,headers) request = urllib2.Request(url, headers) response = urllib2.urlopen(request) page = response.read() # proxy代理设置 enable_proxy=True proxy_handler = urllib2.ProxyHandler({&#39;http&#39;:&#39;http://some-proxy.com:8080&#39;}) null_proxy_handler=urllib2.ProxyHandler({}) if enable_proxy: opener = urllib2.build_opener(proxy_handler) else: opener = urllib2.build_opener(null_proxy_handler) urllib2.install_opener(opener)  静态页面获取  使用BeautifulSoup库来处理html页面  soup = BeautifulSoup(&#39;&amp;lt;b class=&amp;quot;boldest&amp;quot;&amp;gt;Extremely bold&amp;lt;/b&amp;gt;&#39;) # Tag tag = soup.b # b是指html中的&amp;lt;b&amp;gt;标签 # 通过.</description>
    </item>
    
  </channel>
</rss>