<!DOCTYPE html>
<html lang="en-US">
    <head>
        

        <title>
            
            
                Python爬虫-Scrapy进阶 | Panda&#39;s Blog
            
        </title>

        <meta name="title" content="Python爬虫-Scrapy进阶 | Panda&#39;s Blog">

        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="chrome=1">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta name="referrer" content="no-referrer-when-downgrade">
        <meta name="generator" content="">
        <base href="https://panda98.github.io">
        <meta name="description" content="Panda&#39;s Blog">
        
        <meta name="author" content="Panda Pan">
        
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@Pan">
        <meta name="twitter:creator" content="@Pan">
        
        <meta property="og:title" content="Python爬虫-Scrapy进阶 | Panda&#39;s Blog">
        <meta property="og:type" content="website">
        <meta property="og:url" content="https://panda98.github.io">
        <meta property="og:image" content="https://panda98.github.io/images/osprey.png">
        <meta property="og:description" content="Panda&#39;s Blog">
        

        
        <link rel="icon" type="image/png" sizes="16x16" href="https://panda98.github.io/images/logo/favicon.ico">
        <meta name="theme-color" content="#FFF">
        

        <link rel="canonical" href="https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-scrapy%E8%BF%9B%E9%98%B6/">
        
        <link rel="stylesheet" href="https://panda98.github.io/styles/main.css" type="text/css">
        
        <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.10.0/styles/github.min.css">
        
    </head>

    <body>
        

        

        <nav class="row middle-xs center-xs">
            <div class="col-xs-6 col-sm-1 logo">
                <a href="https://panda98.github.io"><img src="https://panda98.github.io/images/osprey-logo.png" alt="Panda&#39;s Blog"></a>
            </div>
            
            <div class="col-xs-3 col-sm-2"><h3><a href="https://panda98.github.io/#about">About</a></h3></div>
            
            <div class="col-xs-3 col-sm-2"><h3><a href="https://panda98.github.io/#work">Work</a></h3></div>
            
            <div class="col-xs-3 col-sm-2"><h3><a href="https://panda98.github.io/#blog">Blog</a></h3></div>
            
            <div class="col-xs-3 col-sm-2"><h3><a href="https://panda98.github.io/#contact">Contact</a></h3></div>
            
            <div class="col-xs-6 col-sm-1 nav-toggle">
                <a href="" class="nav-icon" onclick="return false"><img src="https://panda98.github.io/images/icon-menu.png" alt="Open Menu"><img src="https://panda98.github.io/images/icon-x.png" alt="Close Menu" style="display: none;"></a>
            </div>
        </nav>
        <section class="nav-full row middle-xs center-xs">
            <div class="col-xs-12">
                <div class="row middle-xs center-xs">
                    
                    <div class="col-xs-12"><h1><a href="https://panda98.github.io/#about">About</a></h1></div>
                    
                    <div class="col-xs-12"><h1><a href="https://panda98.github.io/#work">Work</a></h1></div>
                    
                    <div class="col-xs-12"><h1><a href="https://panda98.github.io/#blog">Blog</a></h1></div>
                    
                    <div class="col-xs-12"><h1><a href="https://panda98.github.io/#contact">Contact</a></h1></div>
                    
                </div>
            </div>
        </section>
        <main>


    <section class="container">
        <section class="content">
            <h1> Python爬虫-Scrapy进阶 </h1>

            <div class="sub-header">
                January 2018 · 3 minute read
            </div>

            <article class="entry-content">
                

<p><em>参考资料：<a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/items.html">http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/items.html</a></em></p>

<h2 id="items">Items</h2>

<ul>
<li>提供了<strong>类似于字典的API</strong>以及用于声明可用字段的简单语法</li>
</ul>

<h3 id="声明item">声明Item</h3>

<pre><code class="language-python">import scrapy

class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    stock = scrapy.Field()
    last_updated = scrapy.Field(serializer=str)
</code></pre>

<h3 id="item字段">Item字段</h3>

<ul>
<li><code>Field</code>对象指明了每个字段的元数据</li>
</ul>

<h3 id="item相关api">Item相关API</h3>

<ul>
<li>见参考资料</li>
</ul>

<h3 id="item对象">Item对象</h3>

<pre><code class="language-python">class scrapy.item.Item([arg])
</code></pre>

<ul>
<li>返回一个根据给定的参数可选初始化的item</li>
</ul>

<h3 id="字段-field-对象">字段（Field）对象</h3>

<ul>
<li>Field仅仅是内置的dict类的一个别名，并没有提供额外的方法或属性</li>
<li>即，Field完完全全就是Python字典dict</li>
</ul>

<h2 id="spiders">Spiders</h2>

<ul>
<li><p>Spider类定义了如何爬取某个网站。包括爬取的动作以及如何从网页的内容中提取结构化数据（item）</p></li>

<li><p>spider爬取的循环：</p>

<ol>
<li>以初始的URL初始化Request，并设置回调函数。 当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。</li>
</ol>

<p>spider中初始的request是通过调用 <code>start_requests()</code>来获取的。 <code>start_requests()</code> 读取 <code>start_urls</code>中的URL， 并以 <code>parse</code>为回调函数生成 <code>Request</code> 。</p>

<ol>
<li><p>在回调函数内（parse）分析返回的(网页)内容，返回 <code>Item</code>对象或者 <code>Request</code>或者一个包括二者的可迭代容器。 返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。</p></li>

<li><p>在回调函数内，您可以使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/selectors.html#topics-selectors">选择器(Selectors)</a> (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。</p></li>

<li><p>最后，由spider返回的item将被存到数据库(由某些 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/item-pipeline.html#topics-item-pipeline">Item Pipeline</a> 处理)或使用 <a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/feed-exports.html#topics-feed-exports">Feed exports</a> 存入到文件中</p></li>
</ol></li>
</ul>

<h3 id="spider参数">Spider参数</h3>

<ul>
<li>在cmd中传递Spider参数</li>
</ul>

<pre><code class="language-python">  scrapy crawl myspider -a category=electronics
</code></pre>

<p>​</p>

<ul>
<li>在构造器中获取参数</li>
</ul>

<pre><code class="language-python">  import scrapy

  class MySpider(Spider):
      name='myspider'

      def __init__(self,category=None,*args,**kwargs):
          super(MySpider,self).__init__(*args,**kwargs)
          self.start_urls=['http://www.example.com/categories/%s' % category]
</code></pre>

<h3 id="内置spider参考手册">内置Spider参考手册</h3>

<ul>
<li>Scrapy提供多种通用spider用于继承使用

<ul>
<li>例如，根据某些滚则跟进某个网站的所有链接、根据Sitemaps来进行爬取，或者分析xml/csv源</li>
</ul></li>
</ul>

<h4 id="spider">Spider</h4>

<ul>
<li>是最简单的spider</li>
<li>每个其他的spider必须继承自该类</li>
<li>并没提供什么特殊内容，仅仅请求给定的start_requests，并根据返回的结果调用spider的parse方法</li>
</ul>

<h5 id="start-requests">start_requests()</h5>

<ul>
<li>必须返回一个可迭代对象（iterable），该对象包含了spider用于爬取的第一个Request</li>
<li>默认使用start_urls的url生成Request</li>
</ul>

<h4 id="crawlspider">CrawlSpider</h4>

<ul>
<li>爬取一般网站常用的spider，其定义了一些规则来提供跟进link的方便的机制</li>
<li>除了Spider继承的属性外，还提供了一个新属性

<ul>
<li>rules</li>
<li>一个包含一个(或多个) <code>Rule</code> 对象的集合(list)。 每个 <code>Rule</code>对爬取网站的动作定义了特定表现</li>
</ul></li>
</ul>

<h5 id="爬取规则-crawling-rules">爬取规则（Crawling rules）</h5>

<pre><code class="language-python">class scrapy.contrib.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)
</code></pre>

<ul>
<li>link_extractor：一个link extractor对象，定义了如何从爬取的页面提取链接</li>
<li>callback：一个callable或string。从link_extractor中每获取到链接时将会调用该函数。该毁掉函数接收一个response作为其第一个参数，并返回一个包含Item或Request对象或包含这两者的子类的列表

<ul>
<li><strong>当编写爬虫规则时，请避免使用 <code>parse</code> 作为回调函数。 由于 <code>CrawlSpider</code> 使用 <code>parse</code> 方法来实现其逻辑，如果 您覆盖了 <code>parse</code> 方法，crawl spider 将会运行失败。</strong></li>
</ul></li>
<li>cb_kwargs：包含传递给回调函数的参数的字典</li>
<li>follow：是一个boolean值，指定个根据规则提取的链接是否需要跟进。如果callback为None，follow默认设置为True，否则默认为False</li>
<li>process_links：是一个callable或string(该spider中同名的函数将会被调用)。 从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。</li>
<li>process_request：是一个callable或string(该spider中同名的函数将会被调用)。 该规则提取到每个request时都会调用该函数。该函数必须返回一个request或者None。 (用来过滤request)</li>
</ul>

<h5 id="样例">样例</h5>

<pre><code class="language-python">import scrapy
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors import LinkExtractor

class MySpider(CrawlSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com']

    rules = (
        # 提取匹配 'category.php' (但不匹配 'subsection.php') 的链接并跟进链接(没有callback意味着follow默认为True)
        Rule(LinkExtractor(allow=('category\.php', ), deny=('subsection\.php', ))),

        # 提取匹配 'item.php' 的链接并使用spider的parse_item方法进行分析
        Rule(LinkExtractor(allow=('item\.php', )), callback='parse_item'),
    )

    def parse_item(self, response):
        self.log('Hi, this is an item page! %s' % response.url)

        item = scrapy.Item()
        item['id'] = response.xpath('//td[@id=&quot;item_id&quot;]/text()').re(r'ID: (\d+)')
        item['name'] = response.xpath('//td[@id=&quot;item_name&quot;]/text()').extract()
        item['description'] = response.xpath('//td[@id=&quot;item_description&quot;]/text()').extract()
        return item
</code></pre>

<p>该spider将从example.com的首页开始爬取，获取category以及item的链接并对后者使用 <code>parse_item</code>方法。 当item获得返回(response)时，将使用XPath处理HTML并生成一些数据填入 <code>Item</code>中。</p>

<h4 id="xmlfeedspider">XMLFeedSpider</h4>

<pre><code>from scrapy import log
from scrapy.contrib.spiders import XMLFeedSpider
from myproject.items import TestItem

class MySpider(XMLFeedSpider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = ['http://www.example.com/feed.xml']
    iterator = 'iternodes' # This is actually unnecessary, since it's the default value
    itertag = 'item'

    def parse_node(self, response, node):
        log.msg('Hi, this is a &lt;%s&gt; node!: %s' % (self.itertag, ''.join(node.extract())))

        item = TestItem()
        item['id'] = node.xpath('@id').extract()
        item['name'] = node.xpath('name').extract()
        item['description'] = node.xpath('description').extract()
        return item
</code></pre>

<p>简单来说，我们在这里创建了一个spider，从给定的 <code>start_urls</code> 中下载feed， 并迭代feed中每个 <code>item</code> 标签，输出，并在 <code>Item</code>中存储有些随机数据。</p>

<h4 id="sitemapspider">SitemapSpider</h4>

<ul>
<li>使爬取网站时可以通过Sitemaps来发现爬取的url</li>
<li>其支持嵌套的sitemap，并能从robots.txt中获取sitemap的url</li>
</ul>

<h5 id="sitemap-urls">sitemap_urls</h5>

<ul>
<li>包含要爬取的url的siemap的url列表</li>
<li>也可指定为一个robots.txt，spider会从中分析并提取url</li>
</ul>

<h5 id="sitemap-rules">sitemap_rules</h5>

<ul>
<li><p>一个包含（regex，callback）的元组的列表</p>

<ul>
<li><p>regex使一个用于匹配从sitemap提供的url的正则表达式</p></li>

<li><p>callback指定了匹配正则表达式的url的处理函数</p></li>

<li><p>如</p></li>
</ul>

<pre><code>sitemap_rules = [('/product/', 'parse_product')]
</code></pre>

<ul>
<li>忽略该属性，sitemap中发现的所有url将会被parse函数处理</li>
</ul></li>
</ul>

<h5 id="sitemap-follow">sitemap_follow</h5>

<ul>
<li>一个用于匹配要跟进的sitemap的正则表达式的列表(list)。其仅仅被应用在 使用 Sitemap index files 来指向其他sitemap文件的站点。</li>
<li>默认情况下所有的sitemap都会被跟进。</li>
</ul>

<h5 id="sitemap-alternate-links">sitemap_alternate_links</h5>

<ul>
<li>指定当一个 <code>url</code> 有可选的链接时，是否跟进。 有些非英文网站会在一个 <code>url</code> 块内提供其他语言的网站链接。</li>
</ul>

<h2 id="item-pipiline">Item Pipiline</h2>

<ul>
<li>当Item在Spider中被收集之后，它将会被传递到Item Pipeline，一些组件会按照一定的顺序执行对Item的处理</li>
<li>Item pipeline组件是实现了简单方法的Python类</li>
<li>典型应用：

<ul>
<li>清理html数据</li>
<li>验证爬取的数据（检查item包含某些字段）</li>
<li>查重（并丢弃）</li>
<li>将爬取结果保存到数据库中</li>
</ul></li>
</ul>

<h3 id="编写item-pipeline">编写item pipeline</h3>

<p>必须实现以下方法：</p>

<ul>
<li>process_item(self,item,spider)

<ul>
<li>每个item pipeline组件都需要调用该方法</li>
<li>必须返回一个Item对象或是抛出DropItem异常</li>
</ul></li>
</ul>

<p>也可以实现以下方法：</p>

<ul>
<li>open_spider(self,spider)

<ul>
<li>当spider被开启时，该方法被调用</li>
</ul></li>
<li>close_spider(spider)

<ul>
<li>当spider被关闭时，这个方法被调用</li>
</ul></li>
</ul>

<h3 id="item-pipeline样例">Item pipeline样例</h3>

<p>#####1. 验证价格，同时丢弃没有价格的item</p>

<ul>
<li>为不含税（price_excludes_vat属性）的item调整了price属性，同时丢弃了那些没有价格的item：</li>
</ul>

<pre><code class="language-python">  from scrapy.exceptions import DropItem

  class PricePipeline(object):

      vat_factor = 1.15

      def process_item(self, item, spider):
          if item['price']:
              if item['price_excludes_vat']:
                  item['price'] = item['price'] * self.vat_factor
              return item
          else:
              raise DropItem(&quot;Missing price in %s&quot; % item)
</code></pre>

<h5 id="2-将item写入json文件">2. 将item写入JSON文件</h5>

<ul>
<li>该pipeline将所有爬取到的item，存储到一个独立地items.jl文件，每行包含一个序列化为JSON格式的item：</li>
</ul>

<pre><code class="language-python">  import json

  class JsonWriterPipeline(object):

      def __init__(self):
          self.file = open('items.jl', 'wb')

      def process_item(self, item, spider):
          line = json.dumps(dict(item)) + &quot;\n&quot;
          self.file.write(line)
          return item
</code></pre>

<h5 id="3-去重">3. 去重</h5>

<ul>
<li>用于去重的过滤器，丢弃那些已经被处理过的item（假设item有一个唯一的id，但是我们spider返回的多个item中包含有相同id）：</li>
</ul>

<pre><code class="language-python">  from scrapy.exceptions import DropItem

  class DuplicatesPipeline(object):

      def __init__(self):
          self.ids_seen = set()

      def process_item(self, item, spider):
          if item['id'] in self.ids_seen:
              raise DropItem(&quot;Duplicate item found: %s&quot; % item)
          else:
              self.ids_seen.add(item['id'])
              return item
</code></pre>

<h3 id="启用一个item-pipeline组件">启用一个Item Pipeline组件</h3>

<ul>
<li>将Item Pipeline组件的类添加到ITEM_PIPELINES配置</li>
</ul>

<pre><code class="language-python">  ITEM_PIPELINES={
      'myproject.pipelines.PricePipeline':300,
      'myproject.pipelines.JsonWritePipeline':800,
  }
</code></pre>

<ul>
<li>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内</li>
</ul>

            </article>

            <div class="pagination">
                
                    <a href="https://panda98.github.io/blog/python%E7%88%AC%E8%99%AB-scrapy%E5%88%9D%E6%B6%89/">&laquo; Python爬虫-Scrapy初涉</a>
                
                
            </div>
        </section>
        <br>
        <section class="disqus">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        (function() {
            
            
            if (window.location.hostname == "localhost")
                return;
            var disqus_shortname = 'Pan';
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view comments powered by <a href="http://disqus.com/?ref_noscript">Disqus</a>.</noscript>
</section>

    </section>

        </main>
        <footer class="row middle-xs center-xs">
            
            <div class="col-xs-3 col-md-2"><a href="https://github.com/Pan">GitHub</a></div>
            
            
            <div class="col-xs-3 col-md-2"><a href="https://linkedin.com/in/Pan">LinkedIn</a></div>
            
            
            <div class="col-xs-3 col-md-2"><a href="https://twitter.com/Pan">Twitter</a></div>
            
            
            <div class="col-xs-12">
                
                Copyright &copy; 2018 Panda&#39;s Blog.
                
                
                Theme developed by <a href="https://tomanistor.com">Toma Nistor</a>.
                
            </div>
            
            
            <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
            
            <script src="https://panda98.github.io/scripts/main.js" type="text/javascript"></script>
            
            <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.10.0/highlight.min.js"></script>
        </footer>
    </body>
</html>

